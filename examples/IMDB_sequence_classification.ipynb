{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adopted from https://huggingface.co/transformers/custom_datasets.html#seq-imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from cerebro.backend import SparkBackend\n",
    "from cerebro.keras import SparkEstimator\n",
    "from cerebro.storage import LocalStore\n",
    "from cerebro.tune import RandomSearch, GridSearch, hp_choice\n",
    "\n",
    "def read_imdb_split(split_dir):\n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir is \"neg\" else 1)\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fraction = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "#!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using HugginFace DistilBert model with TF a on single node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
    "if sample_fraction < 1.0:\n",
    "    train_texts, _, train_labels, _ = train_test_split(train_texts, train_labels, test_size=1. - sample_fraction)\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.25)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_projector', 'vocab_transform', 'activation_13', 'vocab_layer_norm']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1172/1172 [==============================] - 772s 659ms/step - loss: 0.2756 - acc: 0.8875 - val_loss: 0.2156 - val_acc: 0.9170\n",
      "Epoch 2/3\n",
      "1172/1172 [==============================] - 770s 657ms/step - loss: 0.1395 - acc: 0.9493 - val_loss: 0.2216 - val_acc: 0.9187\n",
      "Epoch 3/3\n",
      "1172/1172 [==============================] - 770s 657ms/step - loss: 0.0840 - acc: 0.9715 - val_loss: 0.2221 - val_acc: 0.9165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4e97c3feb8>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset.batch(16), epochs=3, validation_data=val_dataset.batch(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Using HugginFace DistilBert model with Cerebro for Distributed Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If GPU memory runs out restart the notebook and only run the imports\n",
    "# and Cerebro section (3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the master url (local[1]) to the correct Spark master url.\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[1]\") \\\n",
    "    .appName(\"IMDB Sequence Classification\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels = read_imdb_split('aclImdb/train')\n",
    "if sample_fraction < 1.0:\n",
    "    train_texts, _, train_labels, _ = train_test_split(train_texts, train_labels, test_size=1. - sample_fraction)\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(pd.DataFrame.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'label': train_labels}\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend = SparkBackend(spark_context=spark.sparkContext, num_workers=1, verbose=0)\n",
    "store = LocalStore(\"/users/snakanda/cerista\")\n",
    "\n",
    "# Define more parameters if you want to try more model configurations.\n",
    "search_space = {'lr': hp_choice([5e-5])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator_gen_fn(params):\n",
    "    from tensorflow.keras.layers import Input\n",
    "    from tensorflow.keras.models import Model\n",
    "    from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "    # TFDistilBertForSequenceClassification model is not directly serializable. Hence we recreate the model\n",
    "    # and wrap it using a serializable Keras model. Check `call` method of TFDistilBertForSequenceClassification\n",
    "    # class for more details\n",
    "    distilbert_model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "    \n",
    "    # Inputs\n",
    "    input_ids = Input(shape=(512,), dtype=tf.int64)\n",
    "    attention_mask = Input(shape=(512,), dtype=tf.int64)\n",
    "\n",
    "    hidden_state = distilbert_model.distilbert(input_ids, attention_mask=attention_mask, training=False)[0]\n",
    "    pooled_output = hidden_state[:, 0]\n",
    "    pooled_output = distilbert_model.pre_classifier(pooled_output)\n",
    "    pooled_output = distilbert_model.dropout(pooled_output, training=False)\n",
    "    logits = distilbert_model.classifier(pooled_output)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=params['lr'])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "    CUSTOM_OBJECTS = {'TFDistilBertForSequenceClassification': TFDistilBertForSequenceClassification}\n",
    "    \n",
    "    keras_estimator = SparkEstimator(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_fn,\n",
    "        metrics=['acc'],\n",
    "        batch_size=16,\n",
    "        custom_objects=CUSTOM_OBJECTS)\n",
    "\n",
    "    return keras_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearch(backend, store, estimator_gen_fn, search_space, 3,\n",
    "                         validation=0.25, evaluation_metric='loss',\n",
    "                         feature_columns=['input_ids', 'attention_mask'],\n",
    "                         label_columns=['label'],\n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2020-10-08 22:50:04, Preparing Data\n",
      "CEREBRO => Time: 2020-10-08 22:50:35, Initializing Workers\n",
      "CEREBRO => Time: 2020-10-08 22:50:36, Initializing Data Loaders\n",
      "CEREBRO => Time: 2020-10-08 22:50:36, Launching Model Selection Workload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_transform', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CEREBRO => Time: 2020-10-08 22:50:42, Model: model_0_1602215442, lr: 5e-05\n",
      "CEREBRO => Time: 2020-10-08 23:07:14, Model: model_0_1602215442, Epoch: 1, train_loss: 3.803926835636247, train_acc: 0.9051520824432373, val_loss: 4.237147256769521, val_acc: 0.8874370455741882\n",
      "CEREBRO => Time: 2020-10-08 23:20:26, Model: model_0_1602215442, Epoch: 2, train_loss: 2.160089514045094, train_acc: 0.9497643709182739, val_loss: 4.144298793687028, val_acc: 0.9039672613143921\n",
      "CEREBRO => Time: 2020-10-08 23:33:36, Model: model_0_1602215442, Epoch: 3, train_loss: 0.9819089077227935, train_acc: 0.9791131019592285, val_loss: 4.785402235516373, val_acc: 0.9039672613143921\n",
      "CEREBRO => Time: 2020-10-08 23:33:52, Terminating Workers\n"
     ]
    }
   ],
   "source": [
    "model = grid_search.fit(df)\n",
    "\n",
    "# Or use the follwing method if the data is already materialized.\n",
    "# model = grid_search.fit_on_prepared_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
